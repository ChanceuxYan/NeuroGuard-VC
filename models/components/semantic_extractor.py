# models/components/semantic_extractor.py
"""
è¯­ä¹‰ç‰¹å¾æå–å™¨ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„HuBERTæˆ–Wav2Vec 2.0æ¨¡å‹æå–è¯­ä¹‰ç‰¹å¾
æœ€ç»ˆä¿®å¤ç‰ˆ (V3)ï¼š
1. [è·¯å¾„] å¼ºåˆ¶ä¼˜å…ˆåŠ è½½æœ¬åœ°æŒ‡å®šè·¯å¾„çš„ HuBERT æ¨¡å‹
2. [æ¢¯åº¦] ç§»é™¤ Processorï¼Œæ‰‹åŠ¨å½’ä¸€åŒ–ï¼Œæ‰“é€šæ¢¯åº¦æµ
3. [æ¨¡å¼] é”å®š Eval æ¨¡å¼ï¼Œè§£å†³ requires_grad æŠ¥é”™
4. [é…ç½®] ä¿®æ”¹ Config é˜²æ­¢ detach
"""
import torch
import torch.nn as nn
import os
from transformers import Wav2Vec2Model, HubertModel

HUBERT_MODEL_AVAILABLE = True
TRANSFORMERS_AVAILABLE = True

class SemanticExtractor(nn.Module):
    """
    è¯­ä¹‰ç‰¹å¾æå–å™¨
    æ”¯æŒHuBERTå’ŒWav2Vec 2.0æ¨¡å‹
    """
    def __init__(self, model_type='hubert', model_name=None, freeze=True, unfreeze_last_n_layers=0):
        super().__init__()
        self.model_type = model_type
        self.unfreeze_last_n_layers = unfreeze_last_n_layers
        
        # 1. è·¯å¾„é€»è¾‘ï¼šå¼ºåˆ¶ä¼˜å…ˆä½¿ç”¨æ‚¨æŒ‡å®šçš„æœ¬åœ°è·¯å¾„
        # æ‚¨æä¾›çš„æ–‡ä»¶è·¯å¾„
        target_bin_path = "/home/yanjunzhe/project/WM-V2/NeuroGuard-VC/hubert/pytorch_model.bin"
        target_dir = os.path.dirname(target_bin_path) # è·å–ç›®å½•: .../hubert/
        
        # å¦‚æœæœªæŒ‡å®šåç§°ï¼Œæˆ–æŒ‡å®šçš„åç§°åœ¨æœ¬åœ°å­˜åœ¨ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨æœ¬åœ°
        final_model_path = None
        
        if os.path.exists(target_bin_path):
            print(f"âœ… Found local model at: {target_bin_path}")
            final_model_path = target_dir
        elif model_name is None:
            # å›é€€ï¼šå°è¯•é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ hubert æ–‡ä»¶å¤¹
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            default_local = os.path.join(project_root, "hubert")
            if os.path.exists(os.path.join(default_local, "pytorch_model.bin")):
                final_model_path = default_local
            else:
                final_model_path = "facebook/hubert-large-ls960-ft" # æœ€åå›é€€åˆ° HF
        else:
            final_model_path = model_name

        print(f"Loading Semantic Model from: {final_model_path}")

        # 2. åŠ è½½æ¨¡å‹
        if model_type == 'hubert':
            try:
                self.model = HubertModel.from_pretrained(final_model_path)
            except Exception as e:
                print(f"âš  HubertModel load failed, trying Wav2Vec2Model: {e}")
                self.model = Wav2Vec2Model.from_pretrained(final_model_path)
        elif model_type == 'wav2vec2':
            self.model = Wav2Vec2Model.from_pretrained(final_model_path)
        else:
            raise ValueError(f"Unsupported model_type: {model_type}")

        # 3. [å…³é”®ä¿®å¤] ä¿®æ”¹é…ç½®ä»¥å…è®¸æ¢¯åº¦å›ä¼ 
        # é»˜è®¤ freeze_feature_encoder=True ä¼šå¯¼è‡´ forward æ—¶ detach()
        self.model.config.freeze_feature_encoder = False 
        
        # å…³é—­æ‰€æœ‰ Dropoutï¼Œä¿è¯ç¡®å®šæ€§
        self.model.config.feat_proj_dropout = 0.0
        self.model.config.attention_dropout = 0.0
        self.model.config.hidden_dropout = 0.0
        self.model.config.activation_dropout = 0.0
        self.model.config.mask_time_prob = 0.0
        self.model.config.layerdrop = 0.0

        # 4. å†»ç»“å‚æ•° (ç‰©ç†å†»ç»“)
        if freeze:
            self.model.eval()
            for param in self.model.parameters():
                param.requires_grad = False
            
            # [æ–°å¢] ç­–ç•¥æ€§è§£å†»ï¼šå…è®¸æœ€å N å±‚ Transformer å‚ä¸å¾®è°ƒ
            # è¿™èƒ½æ˜¾è‘—é™ä½ Generator çš„"æ¬ºéª—"éš¾åº¦ï¼Œæå‡ Acc
            if self.unfreeze_last_n_layers > 0 and hasattr(self.model, 'encoder'):
                print(f"ğŸ”“ Unfreezing the last {self.unfreeze_last_n_layers} layers of Semantic Model...")
                # HuBERT/Wav2Vec2 çš„ encoder layers é€šå¸¸åœ¨ model.encoder.layers
                layers = self.model.encoder.layers
                for i in range(1, self.unfreeze_last_n_layers + 1):
                    for param in layers[-i].parameters():
                        param.requires_grad = True
    
    def train(self, mode=True):
        """
        [æ ¸å¿ƒä¿®å¤] é‡å†™ train æ–¹æ³•
        æ‹¦æˆªå¤–éƒ¨çš„ .train() è°ƒç”¨ï¼Œå¼ºåˆ¶ä¿æŒ HuBERT åœ¨ eval æ¨¡å¼
        é˜²æ­¢ transformers å†…éƒ¨æ‰§è¡Œ requires_grad=True å¯¼è‡´ Crash
        """
        super().train(False) # å¼ºåˆ¶è‡ªèº«ä¸º eval
        self.model.eval()    # å¼ºåˆ¶å†…éƒ¨æ¨¡å‹ä¸º eval
        return self

    def forward(self, waveform):
        """
        æå–è¯­ä¹‰ç‰¹å¾
        Args:
            waveform: (B, 1, T) or (B, T)
        """
        # 1. ç»´åº¦è°ƒæ•´
        if waveform.dim() == 3:
            waveform = waveform.squeeze(1) # (B, T)
            
        # 2. [æ ¸å¿ƒä¿®å¤] PyTorch åŸç”Ÿå½’ä¸€åŒ– (æ›¿ä»£ Processor)
        # å¿…é¡»åœ¨ PyTorch å†…éƒ¨åšï¼Œä¸èƒ½è½¬ numpyï¼Œå¦åˆ™æ¢¯åº¦æ–­æµï¼
        # æ˜¾å¼å…è®¸æ¢¯åº¦è®¡ç®— (å³ä½¿åœ¨ eval æ¨¡å¼ä¸‹)
        with torch.set_grad_enabled(True):
            mean = waveform.mean(dim=-1, keepdim=True)
            std = waveform.std(dim=-1, keepdim=True)
            # åŠ ä¸Š 1e-7 é˜²æ­¢é™¤ä»¥é›¶
            input_values = (waveform - mean) / (std + 1e-7)
        
        # 3. æå–ç‰¹å¾
        # æ­¤æ—¶ self.model å¤„äº eval æ¨¡å¼ä¸” freeze_feature_encoder=False
        # æ¢¯åº¦å¯ä»¥ä» hidden_states ç©¿è¿‡æ¨¡å‹å›ä¼ åˆ° input_values
        outputs = self.model(input_values)
        features = outputs.last_hidden_state # (B, T_frame, D)
        
        # 4. è½¬ç½®ä¸º (B, D, T_frame) ä»¥é€‚é… Conv1d
        features = features.transpose(1, 2)
        
        return features

    def get_feature_dim(self):
        return self.model.config.hidden_size